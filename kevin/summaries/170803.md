## Best Docs to Date
- [API usage overview](https://github.com/openstax/napkin-notes/blob/master/kevin/160921_biglearnApis/api_usage.md)
- [BL deployment overview](https://github.com/openstax/napkin-notes/blob/master/kevin/BiglearnArchitectureDeployment.pdf)
- [Drew's presentation](https://docs.google.com/presentation/d/1qoPqBLD4XqOsIfcM6aJH7IaDQRsxxuA6QBLy4GIZy7w/edit#slide=id.p)
- [Kevin's SPARFA Guide](https://github.com/openstax/sparfa-sandbox/blob/master/klb_sparfa_guide/sparfa_guide.pdf)

## Dev/Testing/Release Woes

(continued from last week)

I have been looking at possible solutions
to the development, testing and deployment issues
raised in:
* [Tutor deployment retrospective](https://docs.google.com/document/d/1aLEMaQ5h75_yXwa1CKJ5CBwTx29bpgiyKOEf3OIPswo/edit#heading=h.p5pj3pqfsjke)
* [Tutor staging issues](https://docs.google.com/document/d/12VFc5ObJNl8KkwzYfOVw42O-D8PYYgxRBb06MHPdXok/edit#heading=h.gp1neznpamzs)

as well as the proposed solutions for `-staging`
[here](https://docs.google.com/document/d/12VFc5ObJNl8KkwzYfOVw42O-D8PYYgxRBb06MHPdXok/edit#heading=h.gp1neznpamzs)

There are three main sources of headaches:
* ensuring that only authorized (FERPA-trained OpenStax) personnel can access PII
* ensuring that `-staging` activity doesn't actually affect `-prod` users/data
* ensuring that `-staging` behaves as much like `-prod` as possible

Some FAQs and answers:
* Why not just use fake data?
  * This works for basic testing, but we have seen too many instances where issues arose for the first time in `-prod`.
    Specific examples include encountering unexpected data during migrations and performance issues.
* Why not just de-identify `-prod` data?
  * Based on previous discussions (dating back to before Chris Nuber left),
    it was determined that `Biglearn` data (which is all _de-identified_) should still be considered PII 
    because we can't be sure that it's adequately _anonymized_ 
    (meaning that it cannot be traced back to specific individuals - see the [Netflix Problem](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/)).
    So de-identifying `-prod` data wouldn't get us anything (policy-wise) in terms of security
    and would cause `-staging` behavior to significantly diverge from `-prod`.
    Specific example: unusual characters in student names or free responses being converted automatically to something harmless.
* Why not just use the same `-staging` environment for both fake and `-prod` data?
  * Constantly adjusting the security (as well as Tutor/Biglearn/Accounts config) settings on the `-staging` environment
    is error-prone and hard to verify.  
    Who would be able to say with certainty that the environment is ready, and when it no longer ready?
    How would we sanity-check that the scripts really executed correctly (or were executed at all)?  
    How do we make sure that intern testing accounts are deactivated before `-prod` data is attached?
    How do we know that the PII has been removed from the environment, and that it is ready for non-FERPA use?
* Will all testing now be done on live data?
  * No.  The `-dev` and `-qa` environments will still use fake data, and should allow us to test the vast majority of scenarios.
* How do we know `-staging` won't impact `-prod`?
  * This is nothing new - we've been dealing with the problem for a while now.
    One option (the most promising, IMO) is to sandbox `-staging` via AWS configuration, effectively cutting it off from the outside world.
    We could then enable access to only specific servers (e.g., `-prod` `Exercises`) and redirect emails to a fake account.
    Once these settings are in place, they'd stay in place (because we're not constantly screwing around with them).
    In addition to this firewall approach, we can adjust the configuration of `Tutor`, `Biglearn`, etc., to point to `-staging` or dummy servers.
    `SalesForce` is the trickiest, it seems to me, because we need to use their sandbox features, and I'm not entirely sure how they work or to what extent we can ensure that we don't accidentally hit the real one.
    But again, this is a problem we already have.
* Will PII just be sitting around when we're not actively testing?
  * No.  The plan is to essentially power-down the environment when not actively used to testing.
* How much cost will this add?
  * Very little; conceivably it might even be a savings.
    The cost should be roughly (cost of `-prod`) * (fraction of time used for testing) + (small storage cost).
    If we keep the system on ice most of the time, it should cost less than the current `-staging` environment, I think.

## SPARFA Algorithm Scaling

I was able to adjust AWS autoscaling group capacity
from code running on an AWS instance
(there were some AWS Identity and Access Management changes since last I looked into this).

I am now in the process of translating
my protocol for distributing work between instances
from ruby to python.
This can be used in conjunction with Mike A.'s code
to automatically handle autoscaling events
without manual intervention.
