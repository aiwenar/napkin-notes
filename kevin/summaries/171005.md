## Best Docs to Date
- [API usage overview](https://github.com/openstax/napkin-notes/blob/master/kevin/160921_biglearnApis/api_usage.md)
- [BL deployment overview](https://github.com/openstax/napkin-notes/blob/master/kevin/BiglearnArchitectureDeployment.pdf)
- [Drew's presentation](https://docs.google.com/presentation/d/1qoPqBLD4XqOsIfcM6aJH7IaDQRsxxuA6QBLy4GIZy7w/edit#slide=id.p)
- [Kevin's SPARFA Guide](https://github.com/openstax/sparfa-sandbox/blob/master/klb_sparfa_guide/sparfa_guide.pdf)
- [Kevin's Autoscaling Notes](https://docs.google.com/document/d/1bmn2xYBURE90fiZrdNG5CN28vEBCPJbKukDTbUqntZ4/edit)

## AWS Autoscaling (proof-of-concept)

I've created AWS templates and repo code that:
* launch EC2 instances
* fetch encrypted secrets from S3
* fetch latest code from GH
* configure service ENV variables
* install/start/monitor worker services
* launch an RDS database instance

The remaining tasks are:
* connect worker code to the database
* create parameters to allow multiple environments

Once that is done,
I can autoscale the Biglearn SPARFA workers.

## AWS Aurora Clone Experiments

I haven't done anything on this front.
I'm pretty sure it'll work as advertised, though.

## AWS Aurora (Dante's Take)

I talked to Dante about Aurora Postgres preview.
He seemed mostly indifferent,
but was concerned a bit
that the instances are ~15% more expensive
than their RDS postgres counterparts.
