## Best Docs to Date
- [API usage overview](https://github.com/openstax/napkin-notes/blob/master/kevin/160921_biglearnApis/api_usage.md)
- [BL deployment overview](https://github.com/openstax/napkin-notes/blob/master/kevin/BiglearnArchitectureDeployment.pdf)
- [Drew's presentation](https://docs.google.com/presentation/d/1qoPqBLD4XqOsIfcM6aJH7IaDQRsxxuA6QBLy4GIZy7w/edit#slide=id.p)
- [Kevin's SPARFA Guide](https://github.com/openstax/sparfa-sandbox/blob/master/klb_sparfa_guide/sparfa_guide.pdf)

## Dev/Testing/Release Woes

### Background

I have been looking at possible solutions
to the development, testing and deployment issues
raised in:
* [Tutor deployment retrospective](https://docs.google.com/document/d/1aLEMaQ5h75_yXwa1CKJ5CBwTx29bpgiyKOEf3OIPswo/edit#heading=h.p5pj3pqfsjke)
* [Tutor staging issues](https://docs.google.com/document/d/12VFc5ObJNl8KkwzYfOVw42O-D8PYYgxRBb06MHPdXok/edit#heading=h.gp1neznpamzs)

These issues include:
* migrations taking longer than expected in production
* surprise data in production
* queries taking longer in production than on staging
* the schedule pressures incurred by large releases
* choosing the correct environment to test specific aspects of the system
* quickly applying hotfixes
* protection of PII

We (OpenStax) have previously discussed many of these issues
at varying levels of detail:
* [BL Scale and Sustain](https://docs.google.com/document/d/1fmSlXJ1kwxJHXO9dvSskODixDQYTs6pnQe03mWsHchk/edit#heading=h.wfp1mks4mpv4)
* [Technical Debt in Tutor/BL](https://docs.google.com/document/d/1QjoopHF-VGv2WbLIKT4kYehZ3OKLIgcsn24iaC7Laqw/edit)
* [BL/Tutor Architecture Vision](https://docs.google.com/document/d/1K6v5_Ua_kQvg9UzuRejzqOfsbIfQhw8Zy8iriygyPTw/edit#heading=h.yxvxbjnoy1pg)
* [BL Event Store](https://github.com/openstax/napkin-notes/blob/master/kevin/161201_biglearnEventStore.md)
* pretty much every deployment retrospective

### Rough Thought Process

#### Separation of Development and Deployment Schedules

We want to deploy things:
* when they're ready
  * developed
  * tested
  * QA/UX approved
* on some sort of schedule
* to some or all users

To do this, we will need:
* CI (continuous integration - code is always merged to master)
* CDel (continuous delivery - code is always considered deployable)
* component architecture (to allow pieces to be deployed and analyzed before being switched on)

We don't necessarily want:
* CDep (continuous deployment - code is always immediately deployed)
because we (at least right now) want a human in the loop.

To get CI/CDel we need:
* feature toggles (to allow merging of partially-implemented features)
  * probably on a per-teacher/student or per-course basis (to control access to paid features and allow canary releases)
* fast, reliable and fully-automated test suite (if the tests pass, the code is considered deployable)

#### Fast, Reliable and Fully-Automated Test Suite

Almost everything hinges on having a test suite
that we run frequently
(most of it every time code is merged,
but maybe only once per hour/day
for more extensive integration tests).
Without one, we are doomed 
to slow and painful manual testing,
necessitating giant infrequent releases.

This virtually requires us to do _real_ TDD (Test-Driven Development),
and really it means BDD (Behavior-Driven Development),
which is like TDD at the feature level.
We currently do a fairly reasonable flavor of TDD,
which we'd need to level-up,
but we don't really do BDD
in any meaningful way.

#### Scaling

We keep running into problems
that arise only in production
because of either:
* the number of active users (somewhat rare)
* the size data associated with some specific user/course
* the sheer amount of data in the db

These issues usually boil down
to a specific query taking too long.
These queries are often trying to answer a question from scratch
instead of combining newly-arrived information
with the previous answer to the question,
resulting in increased cpu and memory usage
due to the vast number of records being touched.

We have been solving this problem
by optimizing the queries
(which often works but results in some gnarly code)
and by using ever more powerful (and expensive) AWS instances.

A better approach, IMO, 
would be to combine smaller, incremental queries
with AWS autoscaling.
This would not only allow us to use smaller instances,
but also easily handle spikes in workload
(like those that happen when new comoponents are brought up).

I also think we should take the stance
that if a feature or component doesn't play well with autoscale,
then we shouldn't release it.

Autoscaling is also one way
to get "canary releases",
where a change is rolled out gradually
to different users
(however, this only applies when no migrations are involved
and, in our case, would likely be better handled by feature toggles).

#### Component Architecture

Breaking Tutor/BL up into components
allows each of them to be independently developed, deployed, and scaled.
It also potentially allows us
to seamlessly replace one component with another
without any end-user downtime,
and to assess the status of a new or changed component
under production conditions
before switching it on.

Components also play nicely with autoscaling,
limit risk,
and encourage re-use as new features are added.

#### Consistency Between Staging and Production

This is a tricky problem to solve,
and might be best avoided entirely.

Cloning the production db into a staging environment is slow,
but really the only way to be _mostly_ sure that test results
will replicate what will happen in production.
(The gap comes from data that has been entered into production
after the db was copied to staging.)
If a problem is found with the deployment
(especially in the area of migrations)
the production db needs to be re-copied (slow)
so that the contaminiation from the previous testing
won't impact the testing of the new code.

Although it might seem crazy at first,
I think the solution to this problem
might be to use production as our staging environment.
This will likely be a tough sell but,
if we solve the other deployment issues,
we basically get this option for free.

#### Potential Downsides

The biggest downsides of anything discussed here are:
* keeping data consistent across components
  * solved by "don't change the past"
* getting the necessary automation in place
* planning for large features that span multiple components

### Bottom Line(s)

Our problems are solvable,
but we lack the discipline/desire/know-how to tackle them,
possibly because we prioritize expediency over everything else.
