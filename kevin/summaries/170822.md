## Best Docs to Date
- [API usage overview](https://github.com/openstax/napkin-notes/blob/master/kevin/160921_biglearnApis/api_usage.md)
- [BL deployment overview](https://github.com/openstax/napkin-notes/blob/master/kevin/BiglearnArchitectureDeployment.pdf)
- [Drew's presentation](https://docs.google.com/presentation/d/1qoPqBLD4XqOsIfcM6aJH7IaDQRsxxuA6QBLy4GIZy7w/edit#slide=id.p)
- [Kevin's SPARFA Guide](https://github.com/openstax/sparfa-sandbox/blob/master/klb_sparfa_guide/sparfa_guide.pdf)

## Surprise Tutor/Biglearn Transfer Issue

AFAIK, there have been no new sequence gaps in `Tutor`/`Biglearn`
(presumably due to the increase in the timeouts).

The previous gaps were fixed
by introducing a `NOP` event
that will be ignored.

## Dev/Testing/Release Woes

I ran an experiment
to see how fast (or slow)
data transfers should be inside `Biglearn`.

```
RDS: t2.medium (4GB RAM, 2vCPU) with 100GB SSD
EC2: t2.micro  (1GB RAM, 1vCPU) x20

records/sec:         4000 (200/sec/worker)
RDS CPU utilization: 40%
RDS IOPS:            200-250 (limit 300 based on 100 GB storage)
```
(We could get as high as ~5000 records/sec
before we hit our IOPS limits.)

This means that transferring 10M responses
should take roughly 30-45mins,
assuming no other significant load
is placed on the database server.

## Random SPARFA Server Bugfixes

Amanda and I fixed some minor errors
related to how SPARFA handles exercises
that have been "deleted" from a Course.

This shouldn't be possible, per se,
because there are safeguards in the ecosystem migration code
to prevent this sort of thing.
But apparently that was inconvenient,
so now we need to deal with this case.
The only sensible thing to do in the short term
is ignore responses to these "deleted" exercises.
In the long-term,
we could add a new `meta-ecosystem` to the Course
to handle these weird cases.


